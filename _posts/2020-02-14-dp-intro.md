---
layout: post
title:  "Dynamic Programming"
date:   2020-02-15 19:10:11 -0500
categories: Computer Science
---
Introduction to dynamic programming

# Dynamic Programming

Where does the name come from?

The creator of the field, Richard Bellman, thought it'd be cool. Unfortunately, the name is not as apt as he had hoped - this method of creating algorithms is not so much "dynamic" as it is a form of **bottom-up recursion**.

### Characteristics of Dynamic Programming Problems

**Optimal Substructure**

If the optimal solution of a problem instance $I$ can be expressed in terms of optimal solutions to certain subproblems of $I$, the problem has **Optimal Substructure**.

**Definite Subproblems**

Given some problem instance $I$ with optimal substructure, the set of solutions to the subproblems of $I$, $S(I)$, enables the optimal solution of $I$ to be computed. Often the overarching solution is the last or largest instance of $S(I)$.

**Recurrence Relation**

We can derive a  recurrence relation on the optimal solutions to the subproblems that contribute to $S(I)$, specified in terms of optimal solutions to instances in $S(I)$ or base cases.

**Bottom-Up Computation**

We can compute the optimal solutions to all instances in $S(I)$ by filling in a table of values containing these optimal solutions. When trying to compute any given entry in the table, the optimal solutions to the relevant subproblems should already have been computed.

The final table entry is the solution to $I$.

**Example**: Computing Fibonacci Numbers

As a simple motivating example, consider the well-known Fibonacci recurrence:
$F(n) = F(n-1) + F(n-2)$ for $n\ge2$, with initial parameters $n_0= 0,n_1=1$.

Here is an inefficient recursive solution to the problem:

```python
def BadFIb(n):
 	if n == 0:
        return 0
    else if n == 1:
        return 1
    else return f(n-1) + f(n-2)
```

The recursion tree for this is both difficult to visualize and difficult to analyze. Take it as a fact that the compute time is exponential.

The issue with this algorithm is that, while computing the value of a Fibonacci number involves many overlapping subproblems, we compute the solutions to these subproblems often.

As a comparison, divide-and-conquer algorithms typically solve subproblems of size $n/2$ to solve an instance size $n$, resulting in a recurrence tree of logarithmic depth and polynomial size (pretty decent).

Now, consider the following more efficient algorithm for Fibonacci:

```Python
def BetterFib(n)
	f = []
    f.extend([0,1]) # Iniitalize starting conditions
    for i in range (2, n+1):
		f.append(f[i-1] + f[i-2])
    return f[n]
```

This is a **bottom-up** (iterative) solution to solving a recursive problem with complexity $O(n)$, exponentially better than our previous algorithm.

The **bottom-up** precomputing of solutions to recursive problems is the essence of dynamic programming.

**Example:** 0-1 Knapsack

**INSERT PROBLEM FORMULATION HERE**

We can break the problem up into subproblems in which we consider only some strict subset of the given items. By doing so, we can reduce the complexity of the problem to a question of whether or not we take an item **given the preceding subsets**. Note that this consideration should be able to occur in any order.

Define a function $P[i,m]$ to be the maximum profit using **any** subset of the items $1...i$ with weight limit $m.$ 

We have the following cases:

- **General**: $i\ge2$ and $m\ge w_i$. Because we have $



As an example, consider the following problem instance:

We want to take at most **7kg** worth of goods.

- Camera: 1kg, value 1000$
- Laptop: 3kg, value 2000$
- Necklace: 4kg, 4000$
- Vase: 5kg, 4500$

